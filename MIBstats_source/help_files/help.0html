<p class="P3">Statistical tools for omics sciences: web application</p><p class="P4"> </p><h1 class="Heading_20_1"><a id="a__Abstract"><span/></a>Abstract</h1><p class="P8"></p><p class="Standard_20__28_user_29_"> </p><h1 class="Heading_20_1"><a id="a__Introduction_and_Aims"><span/></a>Introduction and Aims</h1><p class="P8"></p></p><p class="Standard_20__28_user_29_"> </p><p class="Standard_20__28_user_29_"> </p><h1 class="Heading_20_1"><a id="a__Materials_and_methods"><span/></a>Materials and methods</h1><p class="Standard_20__28_user_29_"> </p><h2 class="Heading_20_2"><a id="a__Programming_language"><span/></a>Programming language</h2><p class="P8">The code for the app was written in R language (version 3.4.1, R Core Team, [9]) using RStudio (version 1.0.143, RStudio, Inc. [17]). In addition to R, the code contains a small number of JavaScript [18] and Hypertext Markup Language (HTML) [19] elements. The app code makes use of the S4 object system, which is one of the three object oriented (OO) systems in R [9].</p><p class="Standard_20__28_user_29_"> </p><h2 class="Heading_20_2"><a id="a__Shiny"><span/></a>Shiny</h2><p class="P8">The graphical user interface for the app was created using the web application framework for R called Shiny (version 1.0.3, RStudio [14]). Shiny uses reactive programming [20] and reactive features were utilised throughout the app. The app also made use of Shiny modules, which enable to make Shiny code encapsulated [21].</p><p class="Standard_20__28_user_29_"> </p><h2 class="Heading_20_2"><a id="a__R_packages"><span/></a>R packages</h2><p class="P8">R packages that were used were the following. stats (version 3.4.1 [9]): used for various statistics-related tasks. preprocessCore (version 1.38.1 [22]): used for quantile normalisation. cowplot (<a id="__DdeLink__631_1923345081"/>version 0.8.0 [23]): used for modifying the look of ggplot2 [24] figures. vsn (version 3.44.0 [25]): used for variance stabilising normalisation. rlist (version 0.4.6.1 [26]): used for performing operations on lists. reshape (version 0.8.7 [27]): used for restructuring data in order to plot it. shinyBS (version 0.61 [28]): used for additional user interface (UI) elements, such as tooltips. ggfortify (version 0.4.1 [29]): used for plotting principal component analysis (PCA) [30] data. plotly (version 4.7.1 [31]): used for generating PCA plots and volcano plot.<a id="__DdeLink__638_1923345081"/> Rmpfr (version 0.6.1 [32]): used for arbitrary precision floating point numbers in iGA. shinyjs (version 0.9.1 [33]): used for performing JavaScript operations within the Shiny app (enabling and disabling buttons). tools (version 3.4.1 [9]): used for getting file extensions from path names. ggplot2 (version 2.2.1.9000 [24]): used for plotting diagrams. gplots (version 3.0.1 [34]): used for generating a heat map via the heatmap.2 function. RankProd (version 3.2.0 [12]): used for calculating rank product. LMGene (v2.28.0 [35]): used for generalised log transformation function. dplyr (0.7.2 [36]): used the “funs” function to create a list of functions calls. data.table (1.10.4, [37]): used for transposing tables. grid (3.4.1 [9]): used in a function that plots multiple ggplot2 [24] graphs on one plot. plyr (1.8.4 [38]): used for processing list elements. DT (0.2 [39]): used to generate DataTables.</p><p class="P8">For button icons, the Font Awesome library [40] was used.</p><p class="Standard_20__28_user_29_"> </p><h2 class="Heading_20_2"><a id="a__iGA_and_db-iGA_scripts"><span/></a>iGA and db-iGA scripts</h2><p class="P8">The R scripts for performing iGA [11] and db-iGA were obtained from Francesco Del Carratore (Manchester Institute of Biotechnology). The scripts contained the iGA and db-iGA algorithms but had no graphical user interface.</p><p class="Standard_20__28_user_29_"> </p><h2 class="Heading_20_2"><a id="a__Diagrams_for_the_report"><span/></a>Diagrams for the report</h2><p class="P8">Diagrams of Shiny modules and S4 classes were created for the report using Inkscape (version 0.92) [41] and Python (version 3.6.2) [42].</p><p class="Standard_20__28_user_29_"> </p><h2 class="Heading_20_2"><a id="a__Statistical_methods"><span/></a>Statistical methods</h2><h3 class="Heading_20_3"><a id="a__PCA"><span/></a>PCA</h3><p class="P8">PCA is a dimensionality reduction technique that is used to identify patterns in multivariate data [30, 43, 44]. The input data for PCA is a multidimensional quantitative dataset that typically consists of a high number of inter-correlated variables [30, 43]. PCA uses orthogonal transformation of input data and produces a set of linearly uncorrelated variables called principal components [44, 45]. PCA aims to explain as much variance in the data as possible with the lowest possible number of components [30, 45].</p><p class="P8">In this app, PCA is used to help users detect outliers  in the data. PCA in the app is implemented using the <span class="T3">prcomp</span> method from R stats package [9]. When running the <span class="T3">prcomp</span> method in the app, the <span class="T3">center</span> and <span class="T3">scale</span> arguments (which control zero centering and unit variance scaling before running PCA) are set to <span class="T3">TRUE</span>. Zero centering and scaling that are done as a part of the PCA procedure in the app only affect the PCA plot and are not applied to the output data of the PCA module of pre-processing.</p><p class="Standard_20__28_user_29_"> </p><h3 class="Heading_20_3"><a id="a__k-means_clustering"><span/></a>k-means clustering</h3><p class="P8">k-means clustering is an unsupervised learning algorithm that can be used to classify a data set [46-48]. k-means clustering partitions the data into a predetermined number of clusters (represented by the variable k), where each data point belongs to the cluster with the nearest mean [46-48]. The aim of k-means clustering is to minimise intra-cluster variance and this is achieved through an iterative process [46-48]. As the first step, k-means clustering algorithm defines k centroids, each corresponding to one cluster [47, 48]. Next, each data point is associated with its nearest centroid [47, 48]. After this, the centroids are recalculated based on the means of the clusters [47, 48]. Next, the procedure starts over from the association of data points with the most adjacent centroid [47, 48]. The loop is repeated until the centroids stop changing [47, 48].</p><p class="P8">In this app, k-means clustering is performed using the <span class="T3">kmeans</span> method of the R stats package with its default settings [9]. The number of centres is chosen by the user.</p><p class="Standard_20__28_user_29_"> </p><p class="Standard_20__28_user_29_"/><h3 class="Heading_20_3"><a id="a__RSD_filtering"><span/></a>RSD filtering</h3><p class="P8">Relative standard deviation (RSD), also known as the coefficient of variation (CV) is calculated using the following formula:</p><p class="P8">100 * s / |x̄|</p><p class="P8">Where:</p><p class="P8">s = standard deviation of the sample</p><p class="P8">x̄ = sample mean [49].</p><p class="P8">RSD filtering removes variables from a dataset if their RSD in quality control samples exceeds a user-defined threshold. This type of filtering can be used to remove variables whose values are unreliable due to technical problems during data collection. The method for RSD filtering was coded for this app using R base functions. The RSD filter in the app requires at least 3 quality control samples in order to perform the filtering. Filtering is cancelled with an error message if it would cause less than 4 variables to remain in the dataset.</p><p class="Standard_20__28_user_29_"> </p><h3 class="Heading_20_3"><a id="a__IQR_filtering"><span/></a>IQR filtering</h3><p class="P8">InterQuartile Range (IQR) of a dataset is its 1st quartile subtracted from the 3rd quartile [50, 51]. IQR can be used to detect outliers in data [50, 51]. Outliers are typically defined based on IQR as observations whose value is less than Q1 - 1.5 IQR or more than Q3 + 1.5 IQR [50, 51]. IQR filtering eliminates values in the data by the IQR of a user-defined set of variables [51]. IQR is calculated separately for each variable in the dataset [51]. In IQR filtering, typically the values in data that differ from the variable mean by more than 1.5*IQR are marked for deletion [51, 52]. The IQR filter method was coded for this app using R base functions [9].</p><p class="Standard_20__28_user_29_"> </p><p class="P2"/><h3 class="Heading_20_3"><a id="a__Mean_centering"><span/></a>Mean centering</h3><p class="P8">Mean centering of data is performed by subtracting the average of each variable from the values of the variable [53-55]. As a result of this subtraction, in mean-centered data, the mean of each variable is zero [53, 54]. Mean centering is often used as a data pre-processing step [53, 54]. The mean centering method in this app was coded using R base functions [9].<br></p><p class="Standard_20__28_user_29_"> </p><h3 class="Heading_20_3"><a id="a__Unit_variance_scaling"><span/></a>Unit variance scaling</h3><p class="P8">Scaling of data is a pre-processing step that is used to adjust for the differences in the value ranges of variables by converting the data into values relative to a scaling factor [54, 55]. Unit variance scaling (also known as autoscaling) uses standard deviation as the scaling factor [54, 55]. After unit variance scaling, all variables have a standard deviation of one [54, 55]. A disadvantage of unit variance scaling is that it can cause amplification of measurement errors [54, 56].</p><p class="P8">In this app, unit variance scaling was implemented using the <span class="T3">scale</span> function from R base package [9]. The <span class="T3">center</span> argument in the <span class="T3">scale</span> function in this app is set to <span class="T3">FALSE</span> (meaning that mean centering is not automatically performed together with the scaling) because the user is presented with the option to perform mean centering as a separate step before scaling.</p><p class="Standard_20__28_user_29_"> </p><h3 class="Heading_20_3"><a id="a__Pareto_scaling"><span/></a>Pareto scaling</h3><p class="P8">Pareto scaling is a scaling method that uses the square root of the standard deviation as the scaling factor [55, 56]. Data values after Pareto scaling stay closer to the original values compared to unit variance scaling [55, 56]. The Pareto scaling method in this app was coded using R base functions [9].</p><p class="Standard_20__28_user_29_"> </p><h3 class="Heading_20_3"><a id="a__Quantile_normalisation"><span/></a>Quantile normalisation</h3><p class="P8">Quantile normalisation is a statistical method that removes global differences between samples with the assumption that the global differences are noise caused by technical variability [57]. Quantile normalisation makes the distribution of variable values of each sample identical [57, 58]. This type of normalisation is commonly used in microarray data analysis [57-59]. Quantile normalisation in this app was implemented using <span class="T3">normalize.quantiles</span> function of the preprocessCore package [22].</p><p class="Standard_20__28_user_29_"> </p><h3 class="Heading_20_3"><a id="a__Logarithmic_transformation"><span/></a>Logarithmic transformation</h3><p class="P8">Logarithmic (log) transformation consists of taking the logarithm of each observation [54, 60]. The base of the logarithm can vary depending on the type of the logarithmic transformation [60]. Log transformation can be used to reduce the skewness of distributions [54, 60]. Log transformation of data that has log-normal distribution can make the data fit the assumptions of parametric statistical tests (<span class="T1">e.g.,</span> t-test or analysis of variance (ANOVA) [54, 60]. Log transformation in this app was coded using R base functions [9]. </p><p class="Standard_20__28_user_29_"> </p><h3 class="Heading_20_3"><a id="a__glog_transformation"><span/></a>glog transformation</h3><p class="P8">Generalised log (glog) transformation function is an alternative for the log transformation [61]. glog transformation of a variable y is defined as log(y + sqrt(y^2 + lambda)) [35]. lambda (which is also called the transform parameter) is an experimentally determined constant [35, 62]. lambda can be estimated using <span class="T3">tranest</span> function in the LMGene R package [35]. A problem with log transformations can be that they increase the variance of observations with low values [35, 63]. In contrast, glog transformation stabilises the variance of data across the entire range of values [35, 63]. glog transformation is often used in the pre-processing of data in microarray analysis [62].</p><p class="P8">glog transformation in this app is performed using the <span class="T3">glog</span> function from LMGene R package [35]. </p><p class="Standard_20__28_user_29_"> </p><h3 class="Heading_20_3"><a id="a__Variance_stabilising_normalisation"><span/></a>Variance stabilising normalisation</h3><p class="P8">Statistical methods often require input data that is with constant variance which does not depend on the mean of the data [63]. If the data does not correspond to this assumption, it may be possible to use a transformation to make it fit the requirement [63, 64]. Variance-stabilising transformations aim to apply a function to values in a dataset so that the variability of the values in the transformed dataset is independent from their mean value [64].</p><p class="P8">Variance stabilising normalisation in this app is performed using the R Bioconductor vsn package [25]. The vsn package has been designed for microarray data but is also applicable for data generated using other methods [25]. The vsn package performs normalisation and a variance stabilising data transformation that is based on a model of the dependence of the variance on the mean intensity [25].</p><p class="Standard_20__28_user_29_"> </p><h3 class="Heading_20_3"><a id="a__Shapiro-Wilk_test"><span/></a>Shapiro-Wilk test</h3><p class="P8">The Shapiro-Wilk test is used to assess whether a random sample comes from a normal distribution [65]. The null hypothesis of the Shapiro-Wilk test is that the data is normally distributed [65]. The test calculates a W statistic and reports a p-value [66, 67]. A low value of the W statistic indicates the lack of normal distribution in the data [68]. If the p-value is lower than the chosen statistical significance level, the null hypothesis is rejected and it cannot be assumed that the samples come from a normal distribution [65].</p><p class="P8">In this app, Shapiro-Wilk test is performed using the <span class="T3">shapiro.test</span> function from R stats package [9].</p><p class="Standard_20__28_user_29_"> </p><p class="Standard_20__28_user_29_"> </p><h3 class="Heading_20_3"><a id="a__F-test"><span/></a>F-test</h3><p class="P8">F-tests are statistical tests in which the test statistic follows an F-distribution under the null hypothesis [69, 70]. An F-test can be used to test if two normal populations have the same variance [69, 70]. The null hypothesis in this case is that the variances are equal [70]. The F-test outputs a p-value and the null hypothesis is rejected when the p-value is below the statistical significance threshold [69, 70]. A limitation of the F-test is that it is sensitive to the violation of assumption of normality [71].</p><p class="P8">In this app, the F-test is done using the <span class="T3">var.test </span>method from R stats package [9].</p><p class="Standard_20__28_user_29_"> </p><h3 class="Heading_20_3"><a id="a__t-test"><span/></a>t-test</h3><p class="P8">The t-test is one of the most common statistical hypothesis tests and can be used to check if the means of two datasets are significantly different from each other [72, 73]. In the t-test, the test statistic follows a Student's t-distribution under the null hypothesis [74]. The t-test is used on data with continuous variables [73]. The t-test compares the difference between two means in relation to the variation in the data [75, 76]. The variation is calculated as the standard deviation of the difference between the means. The p-value of the t-test indicates the probability that the difference between the two means is caused by chance [72].</p><p class="P8">A t-test requires the data to meet the following assumptions. Firstly, unpaired t-test assumes that both populations being compared follow a normal distribution [72, 73]. The paired t-test assumes that the differences between pairs are normally distributed [75]. Secondly, the data from the two populations needs to be sampled independently [72]. Thirdly, in Student's t-tests, the variances of the two populations are assumed to be equal [72]. In a modification of Student's t-test which is called Welch's t-test, the variances of the populations do not need to be equal [77]. R [9] uses Welch's t-test instead of Student's t-test by default [78].</p><p class="P8">A t-test has multiple possible uses. It can be used to compare the means of two populations [73]. The null hypothesis is that the means of two populations are equal [75]. Besides that, one-sample t-test can be used to compare a sample mean to a hypothesised or target value [76]. A t-test can be used for paired difference testing [73]. The null hypothesis in this case is that the difference between two responses measured on the same statistical unit has a mean value of zero [76]. In addition, a t-test can be used to determine whether the slope of a regression line differs significantly from 0 [76].</p><p class="P8">t-test can be paired or unpaired [72, 73]. The unpaired samples t-test is used to compare two separate sets of independent and identically distributed samples [72, 73]. Paired samples t-tests (also known as "dependent samples t-tests") are used to compare samples of matched pairs of similar units [72, 73, 76]. This occurs when there are two measurements of the same item or person, taken at different times or places [72, 73, 75, 76]. Paired t-tests usually have greater statistical power compared to unpaired t-tests [74]. This occurs when confounding factors in the study are independent in the two populations but the paired units have similar confounding factors [74].</p><p class="P8">In this app, the t-test is performed using the <span class="T3">t.test</span> method from the stats package of R [9].</p><p class="Text_20_body_20__28_user_29_"> </p><h3 class="Heading_20_3"><a id="a__Mann_Whitney_U_test"><span/></a>Mann-Whitney U test</h3><p class="P8">Similarly with the t-test, the Mann-Whitney U test (also known as the Wilcoxon rank-sum test) can be used to compare two sample means that come from the same population, and to test whether two sample means are equal or not [73]. A difference of the Mann-Whitney U test from the t-test is that it is nonparametric and does not require the data to follow normal distribution [73, 75, 76].</p><p class="P8">In this app, the Mann-Whitney U test is performed using the <span class="T3">wilcox.test </span>method from R stats package [9].</p><p class="Standard_20__28_user_29_"> </p><h3 class="Heading_20_3"><a id="a__Fold_changes"><span/></a>Fold changes</h3><p class="P8">A fold change in bioinformatics is the final value of a variable divided by the initial value [79]. If the data is log2-transformed, the log2 fold change (FC) can be calculated as log2FC = log2(B) - log2(A), where B is the final value and A is the initial value [80]. The fold change can be calculated from log2 fold change as FC = 2^log2FC [60].</p><p class="P8">The method for calculating fold changes for this app was coded using R base functions.</p><p class="Standard_20__28_user_29_"> </p><h3 class="Heading_20_3"><a id="a__Rank_product"><span/></a>Rank product</h3><p class="P8">Rank product is a non-parametric statistical method for comparing ranked lists [81]. It was introduced as a method for detecting differentially expressed genes in microarray data but it can also be used for many other purposes, such the analysis of metabolomics or proteomics data [81, 82]. The output of this method contains p-values which are calculated by the probability mass distribution of the rank product statistic [81, 83]. Rank product has several advantages over t-test based methods: it works well with noisy data and a low number of replicates [81, 82], which are common in biological data analysis. Besides this, rank product enables combining data from multiple experiments from different laboratories and technical platforms for meta-analysis [81].</p><p class="P8">Rank product requires the measurement variance to be approximately equal for all observations [81, 84]. To make the input data fit this requirement, it may be necessary to use a variance stabilisation method as a pre-processing step [81, 84].</p><p class="P8">In this app, rank product is calculated using the R Bioconductor RankProd package, with the <span class="T3">calculateProduct</span> option set to <span class="T3">TRUE</span> [12].</p><p class="Standard_20__28_user_29_"> </p><h3 class="Heading_20_3"><a id="a__Rank_sum"><span/></a>Rank sum</h3><p class="P8">Similarly to the rank product, the rank sum is a non-parametric statistical test for comparing ranked lists and can be used to detect differentially regulated regulated molecules, e.g., genes or metabolites, in molecular profiling experiments [12, 84]. The rank sum algorithm is very similar to the one of rank product but instead of calculating the geometric mean of ranks, rank sum uses the arithmetic mean [12]. Compared to rank product, rank sum is slightly less affected by outliers in the data [12].</p><p class="P8">In this app, rank sum is calculated using the R Bioconductor RankProd package, with the <span class="T3">calculateProduct</span> option set to <span class="T3">FALSE</span> [12].</p><p class="Standard_20__28_user_29_"> </p><h3 class="Heading_20_3"><a id="a__iGA"><span/></a>iGA</h3><p class="P8">Iterative Group Analysis (iGA) is a statistical method that was devised for detecting functional classes of genes that are differentially expressed in a microarray experiment [11].</p><p class="P8">As input, iGA uses a metric of each variable from an experiment (<span class="T1">e.g.</span> fold change values) and a table with functional annotations of the variables (<span class="T1">e.g.</span> Gene Ontology (GO) annotations) [11]. The assumptions that the metric has to fit are the independence of measurements within a group and equal variation between groups [11]. iGA looks for enrichment of an annotation category among the differentially regulated variables [11]. The iGA algorithm uses a comprehensive hypergeometric statistics calculation [11]. iGA outputs the probability of change (PC-value) for each class from the annotations table [11]. The advantage of iGA over non-iterative methods is that it can be utilised with small and noisy datasets. Another useful property of iGA is that it can use input data from different technical platforms and from different laboratories, and is therefore suited for meta-analysis [11].</p><p class="P8">In this app, iGA is performed using an R function written by Francesco Del Carratore (based on a previously published Perl script [11]).</p><p class="Standard_20__28_user_29_"> </p><p class="Standard_20__28_user_29_"> </p><h3 class="Heading_20_3"><a id="a__db-iGA"><span/></a>db-iGA</h3><p class="P8">Double Boundary Iterative Group Analysis (db-iGA) is a modified version of iGA [11, 85]. While regular iGA only looks for the clustering of a functional class at the top and bottom of the ranked metrics list, db-iGA looks for functional class clusters in all possible regions of the list [85].</p><p class="P8">In this app, db-iGA is carried out using an R function written by Francesco Del Carratore, based on a previously published Mathematica script [85]</p><p class="Standard_20__28_user_29_"> </p><h3 class="Heading_20_3"><a id="a__Volcano_plot"><span/></a>Volcano plot</h3><p class="P8">Volcano plot is a scatter plot that displays fold change (usually represented as log2 FC) on the x axis and statistical significance (usually represented as -log10 p-value) on the y axis [80, 86]. Volcano plots are commonly used for viewing the results of large scale biology experiments in order to have an overview of which data points have simultaneously high fold changes and a low p-value [80, 86]. The data points that fit this condition can be identified on the plot based on the plot region where they are located [80, 86].</p><p class="P8">In this app, the code for generating a Volcano plot was written using a combination of R base functions, functions adapted from the website of Stephen Kelly [87] and the Plotly R library [31].</p><p class="P5"> </p><h3 class="Heading_20_3"><a id="a__p-value_correction_for_multiple_testing"><span/></a>p-value correction for multiple testing</h3><p class="P8">The higher the number of independently tested hypotheses is, the higher is the probability of obtaining an apparently statistically significant p-value when the null hypothesis is true, only because of random chance [88, 89]. This is known as the multiple comparisons problem [88, 89]. p-value correction for multiple testing is essential for the biostatistical analysis of any large-scale molecular profiling experiment. It is used to correct p-values, taking the multiple comparisons problem into account [88, 89]. p-value correction for multiple testing was implemented in the app using the <span class="T3">p.adjust</span> function of the stats package [9].</p><p class="Standard_20__28_user_29_"> </p><h1 class="Heading_20_1"><a id="a__Results"><span/></a>Results</h1><p class="Standard_20__28_user_29_"> </p><h3 class="Heading_20_3"><a id="a__Loading_of_data"><span/></a>Loading of data</h3><p class="Standard_20__28_user_29_"><span class="T2">User interface</span></p><p class="P8">In the tab of loading input data, the user can load a data file in comma separated value (.CSV) format. The user has the option to indicate if the loaded data is already log-transformed or not.</p><p class="P8">Input file format: the app expects the samples to be in columns and variables to be in rows. The first row in the input data file should be the header row that contains sample names as column titles. When using default settings of the app, the first column in the data file should contain variable names.</p><p class="P8">The menu for file loading options can be opened by clicking the "File loading options..." button. Throughout the app, the plus sign icon on buttons indicates that the button toggles the opening and closing of an additional panel in the UI. When clicking on the button with the plus icon, the additional panel opens and the button icon changes to a minus icon.</p><p class="P8">Besides sample names, the app needs to have information on which sample classes the samples belong to. The class labels can either be contained in the header of the data file or be loaded from a separate file. If the class labels are in the data file, the column titles of the data file have to contain the sample name, a separator symbol and the class label name. For example, the column titles of three samples from two classes, when using a dot as the separator could look like this: "sampleA.class1, sampleB.class1, sampleC.class2". The symbol of the separator between the sample names and class label names can be changed from the "File loading options..." menu.</p><p class="P8">An alternative way for loading class label names is loading the class labels from a separate file (the option for this is in the "File loading options..." menu). The class labels file should be a .CSV file. The file should contain a header in its first row and row titles as its first column (these cells of the table may also be left empty). The second column of the table should contain class labels of each sample, in the same order as the sample names in the data file.</p><p class="P5"> </p><p class="Standard_20__28_user_29_"><span class="T2">Table: examples of input data file format</span></p><p class="P5"> </p><table border="1" cellspacing="0" cellpadding="0" class="Table1"><colgroup><col width="186"/><col width="186"/><col width="186"/><col width="186"/></colgroup><tr class="Table11"><td style="text-align:left;width:4.249cm; " class="Table1_A1"><p class="Table_20_Contents"> </p></td><td style="text-align:left;width:4.251cm; " class="Table1_A1"><p class="Standard_20__28_user_29_">sampleA.class1</p></td><td style="text-align:left;width:4.249cm; " class="Table1_A1"><p class="Standard_20__28_user_29_">sampleB.class1</p></td><td style="text-align:left;width:4.249cm; " class="Table1_D1"><p class="Standard_20__28_user_29_">sampleC.class2</p></td></tr><tr class="Table11"><td style="text-align:left;width:4.249cm; " class="Table1_A2"><p class="Table_20_Contents">variable1</p></td><td style="text-align:left;width:4.251cm; " class="Table1_A2"><p class="Table_20_Contents">0.2141</p></td><td style="text-align:left;width:4.249cm; " class="Table1_A2"><p class="Table_20_Contents">0.3471</p></td><td style="text-align:left;width:4.249cm; " class="Table1_D2"><p class="Table_20_Contents">0.1832</p></td></tr><tr class="Table11"><td style="text-align:left;width:4.249cm; " class="Table1_A2"><p class="Table_20_Contents">variable2</p></td><td style="text-align:left;width:4.251cm; " class="Table1_A2"><p class="Table_20_Contents">3.5187</p></td><td style="text-align:left;width:4.249cm; " class="Table1_A2"><p class="Table_20_Contents">4.1291</p></td><td style="text-align:left;width:4.249cm; " class="Table1_D2"><p class="Table_20_Contents">3.2212</p></td></tr><tr class="Table11"><td style="text-align:left;width:4.249cm; " class="Table1_A2"><p class="Table_20_Contents">variable3</p></td><td style="text-align:left;width:4.251cm; " class="Table1_A2"><p class="Table_20_Contents">0.3314</p></td><td style="text-align:left;width:4.249cm; " class="Table1_A2"><p class="Table_20_Contents">0.3431</p></td><td style="text-align:left;width:4.249cm; " class="Table1_D2"><p class="Table_20_Contents">0.3001</p></td></tr></table><p class="Standard_20__28_user_29_">A. Example of the format of an input .CSV file where the class labels are contained in the data file header and separated from sample names with a dot.</p><p class="Standard_20__28_user_29_"> </p><table border="1" cellspacing="0" cellpadding="0" class="Table2"><colgroup><col width="140"/><col width="148"/></colgroup><tr class="Table21"><td style="text-align:left;width:3.2cm; " class="Table2_A1"><p class="Table_20_Contents"> </p></td><td style="text-align:left;width:3.387cm; " class="Table2_B1"><p class="Table_20_Contents">Class labels</p></td></tr><tr class="Table21"><td style="text-align:left;width:3.2cm; " class="Table2_A2"><p class="Table_20_Contents">1</p></td><td style="text-align:left;width:3.387cm; " class="Table2_B2"><p class="Table_20_Contents">class1</p></td></tr><tr class="Table21"><td style="text-align:left;width:3.2cm; " class="Table2_A2"><p class="Table_20_Contents">2</p></td><td style="text-align:left;width:3.387cm; " class="Table2_B2"><p class="Table_20_Contents">class1</p></td></tr><tr class="Table21"><td style="text-align:left;width:3.2cm; " class="Table2_A2"><p class="Table_20_Contents">3</p></td><td style="text-align:left;width:3.387cm; " class="Table2_B2"><p class="Table_20_Contents">class2</p></td></tr></table><p class="Standard_20__28_user_29_"> </p><p class="Standard_20__28_user_29_">B. Example of the format of the class labels .CSV file.</p><p class="P5"> </p><p class="P8">After clicking the button "Load data from the selected file(s)", if the file is successfully loaded, a preview table of the loaded data is shown, so the user can check if the file was read the way the user intended (<span class="T1">e.g.</span> if the matching of class labels with samples is correct). In order to make sure the preview loads quickly, the preview table only shows the first 8 rows and columns of the table. The full data table can be viewed in the next tab.</p><p class="P8">Below the preview table are buttons that enable the user to view sample and variable statistics, such as the number of samples, variables and class labels. The number of "not available" (NA) values in the data is displayed for both samples and variables. For variables, their minimum, maximum and mean values are also shown.</p><p class="P8">Clicking the "Next step" button will confirm the loading of the data table and make the app to proceed to the page where the full data table can be viewed.</p><p class="Standard_20__28_user_29_"> </p><p class="Standard_20__28_user_29_"><span class="T2">Server part</span></p><p class="P8">The loading of input data is handled by the Shiny [14] module called <span class="T3">data_loading_module</span>. When loading a file, error checking takes place to make sure the file is accessible to the app and that it has the correct extension (functions for this are located in the <span class="T3">file_reading_utilities.R </span>file). The data from the file is then loaded into an object from the class <span class="T3">input_data_checker</span>, which has methods for checking that the rows and columns of the loaded .CSV files fit the format requirements of the app. The <span class="T3">input_data_checker </span>class also contains methods for preparing a short summary of the loaded data, in order to display it in the app. If the file is successfully loaded into an <span class="T3">input_data_checker </span>object, the data is transferred to an object from the class <span class="T3">input_data_table</span>. <span class="T3">input_data_table </span>class contains slots for both the data and class labels along with some additional information on the loaded data. The <span class="T3">input_data_table </span>class also contains various methods that can be applied on the data it contains, <span class="T1">e.g.</span> methods for converting the data to different formats. When the user clicks the "Next step" button at the bottom of the data loading page, <span class="T3">data_loading_module</span> returns the <span class="T3">input_data_table</span> object to the shared variable space of the Shiny [14] server. When the <span class="T3">input_data_table</span> object is received in the shared space, it triggers the initialisation of the modules for pre-processing and setting up a statistical test. A copy of the <span class="T3">input_data_table</span> object is made before moving on to pre-processing, making it possible to cancel pre-processing and revert back to the unmodified data.</p><p class="Standard_20__28_user_29_"> </p><h3 class="Heading_20_3"><a id="a__Viewing_loaded_data"><span/></a>Viewing loaded data</h3><p class="Standard_20__28_user_29_"><span class="T2">User interface</span></p><p class="P8">On the page of viewing the loaded data, the full data table is displayed. From the page of loading the data it is possible to proceed to the pre-processing pipeline or to skip pre-processing and move on to the page of setting up statistical tests.</p><p class="Standard_20__28_user_29_"> </p><p class="Standard_20__28_user_29_"><span class="T2">Server part</span></p><p class="P8">This part of app is handled by the <span class="T3">viewing_loaded_data_module</span>, which is a small module that takes an <span class="T3">input_data_table</span> object as input and displays its data in a Shiny [14] DataTable.</p><p class="Standard_20__28_user_29_"> </p><h2 class="Heading_20_2"><a id="a__Pre-processing"><span/></a>Pre-processing</h2><p class="P8">The pre-processing pipeline of the app consists of five main sections, the first of which is the handling of missing (NA), zero and infinite (Inf) values. The pre-processing sections in the pipeline have a fixed order in the app. Throughout the pre-processing pipeline, the user first applies changes to a preview copy of the data and can then examine the changes and either confirm or cancel them. This is in order to make it possible for the user to try a pre-processing step and undo the changes if the result is not as intended, without having to start over from loading the data.</p><p class="Standard_20__28_user_29_"> </p><h3 class="Heading_20_3"><a id="a__Handling_NA__zero_or_infinite_values"><span/></a>Handling NA, zero or infinite values</h3><p class="Standard_20__28_user_29_"> </p><p class="Standard_20__28_user_29_"><span class="T2">User interface</span></p><p class="P8">The handling of NA, zero or infinite values in the data may be necessary because some downstream analysis steps may be incompatible with data containing these values.</p><p class="P8">The page of pre-processing of NA, zero or infinite values offers radio button choices to the user. It is possible to replace zero, -Inf or +Inf values with NAs. There are also options for replacing selected values with the minimum or maximum detected value. With the default settings, the app finds the minimum and maximum values from the data values of loaded data (NA and infinite values not included). If the user knows the detection limits of the measuring instrument that was used to generate the data, it is possible to manually specify the minimum and maximum detectable values. For NAs, the options are to remove variables if their percentage of NA values exceeds a specific threshold, to substitute NAs with zeros or to substitute them with the minimum or maximum detected value.</p><p class="P8">The options to substitute NAs with a manually entered minimum or maximum value can be utilised by the user to replace NAs with any value of choice.</p><p class="P8">A click on the "Apply the selected changes to preview data" button causes the selected changes to be applied to the preview copy of the data. Clicking the "View preview of this pre-processing step" button opens a subpanel that lists the changes made to the preview copy of the data and also shows the result as a data table. The table cells where values have changed are highlighted in yellow. If the pre-processing step causes deletion of rows, the deleted rows are removed from the preview table instead of highlighting them in yellow.</p><p class="P8">The "Reset the preview of this step" button resets the changes made to the preview copy of the data. The last two buttons on the page enable the user to either apply the previewed changes to input data or go to the next pre-processing step without making changes to the data.</p><p class="Standard_20__28_user_29_"> </p><p class="Standard_20__28_user_29_"><span class="T2">Server part</span></p><p class="P8">The handling of NA, zero and infinite values is performed in the <span class="T3">handling_na_zero_inf_values_module</span>. The module takes an <span class="T3">input_data_table</span> object as input and returns a modified version of this object, if the user applies any changes in this pre-processing step. The input and output of all other pre-processing modules in this app have been arranged in the same way.</p><p class="P8">The module uses an object from the <span class="T3">pre-processing_na_zero_inf </span>class. The <span class="T3">pre-processing_na_zero_inf </span>objects can take an <span class="T3">input_data_table </span>object as input and then replace values with other values in its data. The <span class="T3">pre-processing_na_zero_inf </span>class contains a set of methods for dealing with NA, zero and infinite values (coded from scratch using R base functions). The <span class="T3">pre-processing_na_zero_inf </span>object returns the modified <span class="T3">input_data_table</span> object and a list that contains a report of the changes that were made. </p><p class="Standard_20__28_user_29_"> </p><h3 class="Heading_20_3"><a id="a__PCA"><span/></a>PCA</h3><p class="Standard_20__28_user_29_"><span class="T2">User interface</span></p><p class="P8">In this step of pre-processing, PCA can be used to detect and remove samples or variables that are outliers in the data. When the PCA page is opened, the user has the choice of either running PCA or skipping to the next pre-processing step. If PCA is run, PCA scores and loadings plots will be shown, along with further options. The cumulative variance explained by each principal component can be seen in the diagram that opens when the "Show PCA scree plot" button is clicked. The PCA algorithm calculates 20 principal components. The user can choose which principal components to view in the scores and loading plots from the dropdown menus under the scores plot. In the scores plot, each data point corresponds to a sample. When using default settings, the samples in this plot are coloured based on their class labels. In the loadings plot, each data point represents a variable. By clicking and dragging the mouse on the scores and loading plots, it is possible to select samples and variables. The text boxes under these plots show the names of the samples and variables that are currently selected. Based on the locations of data points on the scores and loadings plots, outliers can be identified (outliers are located far away from the rest of the data points). When the outliers have been selected on the plot, the user can click the button labelled as "Preview: remove the samples/variables marked for deletion and rerun PCA". This will result in deletion of the selected variables and samples from the preview copy of the data. After this, PCA will be redone with the modified data. The deletion of samples and variables can be done multiple times. However, the number of samples or variables cannot be reduced to be lower than 3. The preview copy of data can be reset by clicking the button labelled as "Preview: reset the changes to samples and variables".</p><p class="P8">As an alternative to colouring samples in the scores plot by their class labels, the app offers the option to colour them by groups that result from k-means clustering of the data. This may be useful for checking if patterns of sample similarity that appear in the data follow the distribution of class labels or not.</p><p class="P8">The mode of colouring the samples can be toggled from the "Change the mode of colouring samples..." button. The clustering settings (number of clusters and the choice of principal components used in the clustering) can be changed from the menu that opens from the "Clustering settings..." button. The borders of the clusters assigned to the samples by the k-means algorithm can be viewed by clicking the "Show k-means clustering plot" button.</p><p class="P8">To view the scores and loadings of more than two principal components at once, the app has the option to generate a plot panel of multiple combinations of principal components. This can be accessed by clicking the "Show a plot panel of multiple combinations of PCs" button. The number of principal components to display can be chosen from a dropdown menu and the app generates a figure that shows the scores and loadings plots of all combinations of the selected number of principal components.</p><p class="P8">At the bottom of the PCA page are the buttons for applying or cancelling the previewed changes.</p><p class="Standard_20__28_user_29_"> </p><p class="Standard_20__28_user_29_"> </p><p class="Standard_20__28_user_29_"><span class="T2">Server part</span></p><p class="P8">In this app, PCA is performed in the module called <span class="T3">preproc_pca_module</span>. The module can be run in two modes: either with the complete interface (with the scree plot, k-means clustering options and the button for deleting variables and samples) or with a minimal interface that only contains the PCA scores and loadings plot. The mode is controlled by the <span class="T3">extended_options</span> variable in the module. In the app, there are two instances of the PCA module: the one that serves as the second step of the pre-processing pipeline is with full interface, while the one that is on the page of viewing pre-processed data is with the minimal interface. In the <span class="T3">preproc_pca_module</span>, PCA is performed using and object of the <span class="T3">pre-processing_pca </span>class. The <span class="T3">pre-processing_pca </span>object can take an <span class="T3">input_data_table</span> object as its input and has methods for running PCA (using prcomp [9]) and generating ggplot2 [24] plots of the PCA results. Other plots (<span class="T1">e.g.</span> the Plotly [31] scores and loadings plot or the k-means plot) are generated in the <span class="T3">preproc_pca_module</span> module using the PCA statistics returned by the object from the <span class="T3">pre-processing_pca </span>class.</p><p class="Standard_20__28_user_29_"> </p><h3 class="Heading_20_3"><a id="a__Sorting_of_data"><span/></a>Sorting of data</h3><p class="Standard_20__28_user_29_"><span class="T2">User interface</span></p><p class="P8">The first section of the page of data sorting enables the user to delete samples or variables by choosing them by their names from grouped checkboxes.</p><p class="P8">The next section is for reordering of samples. Here, two lists of samples are displayed: the list on the left is the old order of samples and the list on the right is the new order. Initially, the old order list is full and the new order list is empty. By left-clicking on the items of the old order list, the elements of the list can be selected. Multiple selection is allowed. By clicking on the lines of the new order list, the slots in that list can be selected. Multiple selection is also allowed in the new order list but the number of selected elements cannot exceed the number of elements selected in the old order list. By clicking the "Move from old list to new list" button, samples that are selected in the old order list can be moved to the selected slots in the new order list. Once the new order list is full, the order can be applied to the preview copy of the data. There is also a button to reset the sample reordering to its initial state.</p><p class="P8">As an alternative way of reordering samples, the order of samples can also be loaded from a text file.</p><p class="P8">The third section on the data sorting page is for modifying class labels. This section displays a modifiable list of class labels, along with the sample names associated with each list item. For reference, there is also a column that shows unmodified class labels (as they are in the input data file that was loaded). This is to aid keeping track of the changes made when modifying class labels. The elements of the "New class labels" list can be selected by clicking on them. The selected elements can be substituted by choosing a class label from the "Selected class label" dropdown menu and then clicking the button "Apply the selected class label to the selected sample(s)". The subpanel opened by clicking the "Create new class label..." button enables the user to type in new names for class labels. The new names, when added to the list of class labels, will appear in the "Select class label" dropdown menu.</p><p class="P8">At the bottom of the sorting page, there are the buttons for resetting the changes, applying the changes or skipping the pre-processing step.</p><p class="Standard_20__28_user_29_"> </p><p class="Standard_20__28_user_29_"><span class="T2">Server part</span></p><p class="P8">Sorting is done in the module named <span class="T3">pre-processing_sorting_module</span>. This module contains an object from the <span class="T3">pre-processing_sorter</span> class, which has 16 methods for the sorting tasks (coded using R base functions). A <span class="T3">pre-processing_sorter </span>object, similarly with other objects of pre-processing classes, takes an<span class="T3"> input_data_table </span>object as an input and can then modify its data.</p><p class="Standard_20__28_user_29_"> </p><h3 class="Heading_20_3"><a id="a__Filtering_of_data"><span/></a>Filtering of data</h3><p class="Standard_20__28_user_29_"><span class="T2">User interface</span></p><p class="P8">The filtering part of pre-processing consists of an RSD filter and an IQR filter. In the RSD filter part, the user can choose, using checkboxes, which sample classes serve as the quality control classes. The maximum RSD percentage that is allowed in quality control samples can be set from a slider. The RSD filtering is performed when the "Perform RSD filtering" button is clicked. After filtering, a message box will display a short report on the changes that were made to the data.</p><p class="P8">In the IQR filter part of the interface, the user can select the variables to include in IQR filtering from a grouped checkbox. Using a slider, the user can set the value of the multiplier constant used in filtering (the default value is 1.5). In addition, there are radio buttons that offer the user the choice between replacing the values with NA or with the average value of the variable.</p><p class="P8">A click on the "View preview of filtered data" button opens a table that shows the preview copy of the data after filtering. As in the other pre-processing sections, at the bottom of the page there are buttons for applying or cancelling the changes.</p><p class="Standard_20__28_user_29_"> </p><p class="Standard_20__28_user_29_"> </p><p class="Standard_20__28_user_29_"><span class="T2">Server part</span></p><p class="P8">Filtering is done in the <span class="T3">pre-processing_filtering_module</span>, which contains an object from the <span class="T3">pre-processing_filter</span> class. The <span class="T3">pre-processing_filter</span> class contains a method for RSD filtering and a method for IQR filtering. Besides the filtering itself, the code in the filtering methods includes performing checks on the input data and reporting the changes made to the data to the user.</p><p class="Standard_20__28_user_29_"> </p><h3 class="Heading_20_3"><a id="a__Scaling__normalisation_and_variance_stabilisation"><span/></a>Scaling, normalisation and variance stabilisation</h3><p class="Standard_20__28_user_29_"><span class="T2">User interface</span></p><p class="P8">The first step for the user on the page of scaling, normalisation and variance stabilisation is to choose whether to perform mean centering of the data. Next, the user is presented with the mutually exclusive choices of scaling, quantile normalisation, logarithmic transformation and variance stabilising normalisation (or skipping this part of pre-processing). The scaling subpanel offers the choice between unit variance scaling and Pareto scaling. </p><p class="P8">In the log transformation subpanel, the user can select either log transformation (with either 2 or 10 as the base) or glog transformation. Because real-valued logarithm of negative values is undefined [60], the app does not allow log transformation of data with negative values. In contrast with this, the app allows performing glog transformation on data with negative values but issues a warning when negative values are found.</p><p class="P8">For all data transformations on the page, there are box plot diagrams that show variable values before and after each pre-processing step. The last box plot preview button on the page shows the effect of the entire "Scaling, normalisation and variance stabilisation" module on the data.</p><p class="P7"> </p><p class="Standard_20__28_user_29_"><span class="T2">Server part</span></p><p class="P8">Scaling, normalisation and variance stabilising normalisation are performed in the module named <span class="T3">pre-processing_snv_module</span>. This module contains objects from three classes (<span class="T3">pre-processing_scaler</span>, <span class="T3">pre-processing_normaliser</span> and <span class="T3">pre-processing_variance_stabiliser</span>) that cover the different tasks within the module. The <span class="T3">pre-processing_scaler</span> class contains the methods for mean centering, unit variance scaling and Pareto scaling. The <span class="T3">pre-processing_normaliser</span> class has the methods for quantile normalisation, log transformation and glog transformation. The <span class="T3">pre-processing_variance_stabiliser </span>class has the method for variance stabilising normalisation. These three S4 objects are interconnected, so that the output of one object can serve as the input of another one.<span class="T3"> pre-processing_snv_module</span> contains a function (<span class="T3">get_the_result_of_snv_pipeline</span>) that retrieves the combined result of the aforementioned three S4 objects. The ggplot2 [24] box plot previews of the data processing steps in this module are generated in the <span class="T3">preproc_snv_plots</span> module. There are 8 instances of the <span class="T3">preproc_snv_plots m</span>odule running in the app, each corresponding to a box plot preview of a different step on the "Scaling, normalisation and variance stabilisation" page.</p><p class="Standard_20__28_user_29_"> </p><h3 class="Heading_20_3"><a id="a__Viewing_pre-processed_data"><span/></a>Viewing pre-processed data</h3><p class="Standard_20__28_user_29_"><span class="T2">User interface</span></p><p class="P8">The page of viewing pre-processed data enables the user to view the pre-processed data as a PCA plot and as a table. On this page, the PCA plot is only for viewing the results of pre-processing and does not have the options for removing data points.</p><p class="P8">At the bottom of the page, there are buttons for either confirming or cancelling the changes to the data made in the pre-processing pipeline. After making this choice, the user is directed to the page of setting up a statistical test.</p><p class="P7"> </p><p class="Standard_20__28_user_29_"><span class="T2">Server part</span></p><p class="P8">For viewing pre-processed data, the app contains a small module named <span class="T3">viewing_pre-processed_data_module</span>, which displays the pre-processed data as a DataTable. The page of viewing pre-processed data also contains an instance of the <span class="T3">preproc_pca_module</span>. The page has the option to cancel pre-processing. If this is triggered, the <span class="T3">input_data_table</span> object in shared server space is replaced with a copy of it that was made before pre-processing.</p><p class="Standard_20__28_user_29_"> </p><h2 class="Heading_20_2"><a id="a__Statistical_tests"><span/></a><span class="T9">Statistical tests</span></h2><h3 class="Heading_20_3"><a id="a__Choosing_the_data_for_comparing_two_groups"><span/></a>Choosing the data for comparing two groups</h3><p class="Standard_20__28_user_29_"><span class="T2">User interface</span></p><p class="P8">This page is meant for setting up statistical tests that compare two groups of samples. The app allows adding samples with various groupings to a queue and then running all the queued tests at once.</p><p class="P8">At the top of the page, there is the data summary panel, similarly with the data loading page. The next two main sections of the page are the panel of choosing the samples for a test and the panel of the test queue.</p><p class="P8">In the section of choosing samples, the sample classes can be selected from the "Sample class" dropdown menu and added to test group 1 or test group 2. A test group can have samples from multiple classes. However, both test groups cannot contain the same class.</p><p class="P8">Using the "Create new subset from the selected class..." button, it is possible to create sample class subsets where the user can choose what samples to keep and what samples to leave out. If a subset has been created out of a class, it will appear in the "Sample class subset" dropdown menu. Multiple subsets can be made from one class. The "View a list of samples in the subset" button, when clicked, will display a message box that lists the samples that make up the subset that is selected in the "Sample class subset" dropdown menu.</p><p class="P8">The "Add to queued tests" button adds the test with the selected two groups to the test queue. The "Clear the list" button clears the choice of sample classes for the two test groups. The "Test parameters..." button opens a subpanel where the user can add an optional label for the tests, choose if the test is paired or not, or change the type of multiple testing correction that will be applied to p-values.</p><p class="P8">For setting up a paired test, the two test groups need to have an equal number of samples. The number of sample classes in each group of the paired test can be different, as long as the total count of samples in both groups is identical. For paired data, the automatically assigned pairing of samples in this app is performed in the same way as the default behaviour of R when pairing samples. Namely, the pairing algorithm goes through the table from left to right (column by column) and finds the first column that contains group 1 samples and the first column that contains group 2 samples. Samples from the same rows of these two columns are then marked as paired with one another. Next, the pairing algorithm finds the second column contains group 1 samples and the second column that contains group 2 samples. The samples from the same rows are paired with one another again. The process is repeated until all samples in both groups have been assigned pairings.</p><p class="P8">In the test queue part of the page, it is possible to view the pairing of samples in a paired test by clicking on a row of the test queue table that contains a paired test. The pairings will be displayed as a list. Furthermore, there is the option to edit the pairing of samples manually from the subpanel that opens when clicking the button "Edit the pairing of samples" that is located underneath the pairings list.</p><p class="P8"> The section for editing the pairing of samples consists of three tables: group 1 samples table, pairs table and group 2 samples table. The window of the app should be maximised in order for the tables to be displayed properly. Initially, the pairs table will be full and the group 1 samples table and group 2 samples table will be empty. Pairs in the pairs table can be selected and removed by clicking on them and then clicking the "Remove from pairs" button. The samples from the removed pairs will appear in the group 1 samples table and group 2 samples table. Using the buttons "Add group 1 item to pairs" and "Add group 2 item to pairs", the samples can be moved back to the pairs table, to the row that is selected. The row in the pairs table where the samples are added does not need to be the same row where they were originally located and this makes it possible to rearrange the pairings. Once the pairs table is filled with rearranged sample pairs, the new pairing can be applied by clicking the "Apply the new pairing of samples" button.</p><p class="P8">The button "Fill the test queue with all possible pairs of sample classes" under the test queue button can be used to set up pairwise comparisons of all classes in the data with one click. For example, if there are three classes in the data (A, B and C), the queue will be filled with the following tests: A versus (vs) B, A vs C and B vs C.</p><p class="P8">The button "Clear the table" under the test queue table can be used to reset the test queue.</p><p class="P8">If the test queue is not empty, the user can proceed to perform statistical tests with the queued samples by clicking the buttons at the bottom of the page.</p><p class="P7"> </p><p class="P7"> </p><p class="Standard_20__28_user_29_"><span class="T2">Server part</span></p><p class="P8">Setting up a statistical test is done in the module called <span class="T3">setting_up_two_groups_comparison_module</span>. This module takes an <span class="T3">input_data_table</span> object as its only input. The details of each test queue item in the module are stored in an object from the class <span class="T3">two_groups_test_queue</span>. This class also contains methods dealing with the test queue,<span class="T1"> e.g.</span> triggering the running of all the queued statistical tests. The handling of data subsets is done using methods contained in the <span class="T3">input_data_table</span> class and functions in <span class="T3">setting_up_two_groups_comparison_module</span>. The module outputs a list that contains the <span class="T3">two_groups_test_queue</span> object. The list also contains a variable that stores the name of the statistical test button that was clicked by the user (at the bottom of the page of setting up statistical tests).</p><p class="Standard_20__28_user_29_"> </p><h3 class="Heading_20_3"><a id="a__Running_statistical_tests"><span/></a>Running statistical tests</h3><p class="Standard_20__28_user_29_"><span class="T2">User interface</span></p><p class="P8">The page for parameter selection when running a t-test contains the options to toggle the use of Welch's correction and to select between a two-tailed test and two types of one-tailed tests. There is also a slider to set the maximum number of missing values in t-test input data. If the percentage of missing values of a variable exceeds the NA percentage threshold set here, the p-value for the variable will not be calculated. The choice of parameters for Mann-Whitney U test is similar with the t-test, except that there is no Welch's correction option for the Mann-Whitney U test.</p><p class="P8">For the Shapiro-Wilk test and the F-test and for the calculation of fold changes, rank product and rank sum, there are no extra options for the user to choose.</p><p class="Standard_20__28_user_29_"><span class="T2">Server part</span></p><p class="P8">The <span class="T3">two_groups_test_queue</span> object in the shared variable space of Shiny [14] server is updated when changes are made on in <span class="T3">setting_up_two_groups_comparison_module</span>. The running of statistical tests in the app is triggered from the shared variable space of Shiny server, using methods in <span class="T3">two_groups_test_queue </span>object. Most of the tests in the app (the t-test, F-test, rank sum, rank product and fold changes) use a method called <span class="T3">run_two_groups_test</span>. The <span class="T3">run_two_groups_test</span> method in a <span class="T3">two_groups_test_queue</span> object goes through the test queue and triggers the execution of each test queue item. The choice of the statistical test is given as an argument when using the <span class="T3">run_two_groups_test</span> method. For each test queue item, an object from <span class="T3">two_groups_test </span>class is created. The <span class="T3">two_groups_test</span> class contains slots to store the information that is needed to run one statistical test (test type, test parameters and the data contained in the test groups) along with the methods for running these tests. The methods for performing statistical tests are mostly based on previously existing R functions but contain additional code for error trapping and for storing the results in a format that can later be used by other parts of the same app.</p><p class="P8">After running a statistical test, a <span class="T3">two_groups_test </span>object outputs the results as an object belonging to the class <span class="T3">two_groups_test_result</span>.<span class="T3"> </span>The <span class="T3">two_groups_test_result </span>objects are stored as a list inside objects from the class <span class="T3">two_groups_test_results_tracker</span>. There is one <span class="T3">two_groups_test_results_tracker</span> object per each type of statistical test in the app. All the <span class="T3">two_groups_test_results_tracker</span> objects are stored as a list in a reactiveValues variable named <span class="T3">rtr</span> in the shared variable space of Shiny server.</p><p class="Standard_20__28_user_29_"> </p><p class="Standard_20__28_user_29_"> </p><h3 class="Heading_20_3"><a id="a__Performing_iGA_and_db-iGA"><span/></a>Performing iGA and db-iGA</h3><p class="Standard_20__28_user_29_"><span class="T2">User interface</span></p><p class="P8">The user interface options are identical for iGA and db-iGA in this app. iGA [11] and db-iGA require two different kinds of input data: metric data of variables (p-values, fold changes or any other quantitative values) and annotations data of the variables (a boolean matrix that contains information about what functional classes each sample belongs to). The metric data can be retrieved from the p-values or fold changes calculated in this app during the same session. Alternatively, the user can load a .CSV file with metrics. The metrics file should contain a header row and have variable names in the first column and the corresponding metrics in the second column. An optional label can be added to the metrics data by the user: if the data is loaded from a file, the label can be helpful for keeping track of the name and type of the experiment the data is from.</p><p class="Standard_20__28_user_29_"> </p><p class="Standard_20__28_user_29_">Example of the metrics file format:</p><table border="1" cellspacing="0" cellpadding="0" class="Table3"><colgroup><col width="371"/><col width="371"/></colgroup><tr class="Table31"><td style="text-align:left;width:8.5cm; " class="Table3_A1"><p class="Table_20_Contents">Gene</p></td><td style="text-align:left;width:8.498cm; " class="Table3_B1"><p class="Table_20_Contents">p-value</p></td></tr><tr class="Table31"><td style="text-align:left;width:8.5cm; " class="Table3_A2"><p class="Table_20_Contents">MC1R</p></td><td style="text-align:left;width:8.498cm; " class="Table3_B2"><p class="Table_20_Contents">0.2759</p></td></tr><tr class="Table31"><td style="text-align:left;width:8.5cm; " class="Table3_A2"><p class="Table_20_Contents">MC4R</p></td><td style="text-align:left;width:8.498cm; " class="Table3_B2"><p class="Table_20_Contents">0.3412</p></td></tr><tr class="Table31"><td style="text-align:left;width:8.5cm; " class="Table3_A2"><p class="Table_20_Contents">MITF</p></td><td style="text-align:left;width:8.498cm; " class="Table3_B2"><p class="Table_20_Contents">0.1526</p></td></tr></table><p class="Standard_20__28_user_29_"> </p><p class="P8">The annotations file should also be a .CSV file. The annotations file can only be loaded after a metrics file has been loaded. The header of the annotations file should contain the names of annotation classes. The first column should contain the names of variables. The numeric matrix part of the table should contain the value 1 if the variable that corresponds to the row belongs to the functional class that corresponds to the column. If the variable does not belong to that functional class, the numeric matrix value in the given position should be 0.</p><p class="Standard_20__28_user_29_"> </p><p class="Standard_20__28_user_29_">Example of the annotations file format:</p><table border="1" cellspacing="0" cellpadding="0" class="Table4"><colgroup><col width="149"/><col width="148"/><col width="149"/><col width="149"/><col width="149"/></colgroup><tr class="Table41"><td style="text-align:left;width:3.399cm; " class="Table4_A1"><p class="Table_20_Contents">Variable</p></td><td style="text-align:left;width:3.397cm; " class="Table4_A1"><p class="Table_20_Contents">transmembrane_protein</p></td><td style="text-align:left;width:3.399cm; " class="Table4_A1"><p class="Table_20_Contents">GPCR</p></td><td style="text-align:left;width:3.401cm; " class="Table4_A1"><p class="Table_20_Contents">transcription_factor</p></td><td style="text-align:left;width:3.404cm; " class="Table4_E1"><p class="Table_20_Contents">alpha_MSH_receptor</p></td></tr><tr class="Table41"><td style="text-align:left;width:3.399cm; " class="Table4_A2"><p class="Table_20_Contents">MC1R</p></td><td style="text-align:left;width:3.397cm; " class="Table4_A2"><p class="Table_20_Contents">1</p></td><td style="text-align:left;width:3.399cm; " class="Table4_A2"><p class="Table_20_Contents">1</p></td><td style="text-align:left;width:3.401cm; " class="Table4_A2"><p class="Table_20_Contents">0</p></td><td style="text-align:left;width:3.404cm; " class="Table4_E2"><p class="Table_20_Contents">1</p></td></tr><tr class="Table41"><td style="text-align:left;width:3.399cm; " class="Table4_A2"><p class="Table_20_Contents">MC4R</p></td><td style="text-align:left;width:3.397cm; " class="Table4_A2"><p class="Table_20_Contents">1</p></td><td style="text-align:left;width:3.399cm; " class="Table4_A2"><p class="Table_20_Contents">1</p></td><td style="text-align:left;width:3.401cm; " class="Table4_A2"><p class="Table_20_Contents">0</p></td><td style="text-align:left;width:3.404cm; " class="Table4_E2"><p class="Table_20_Contents">1</p></td></tr><tr class="Table41"><td style="text-align:left;width:3.399cm; " class="Table4_A2"><p class="Table_20_Contents">MITF</p></td><td style="text-align:left;width:3.397cm; " class="Table4_A2"><p class="Table_20_Contents">0</p></td><td style="text-align:left;width:3.399cm; " class="Table4_A2"><p class="Table_20_Contents">0</p></td><td style="text-align:left;width:3.401cm; " class="Table4_A2"><p class="Table_20_Contents">1</p></td><td style="text-align:left;width:3.404cm; " class="Table4_E2"><p class="Table_20_Contents">0</p></td></tr></table><p class="Standard_20__28_user_29_"> </p><p class="P8">The annotations file needs to have rows for every variable in the metric file in order to perform iGA or db-iGA. The order of rows in the two files does not need to be the same but the spelling of variable names needs to be identical in order for the app to be able to match them. The annotation file is allowed to contain excess rows with variables that are not present in the metrics file.</p><p class="P8">The app does not currently have the option to generate annotations files automatically, so it is up to the user to prepare the annotations files for the variables of interest (<span class="T1">e.g.</span> based on gene ontology (GO) annotations [90]).</p><p class="P8">The iGA settings subpanel has the option for choosing the direction of calculation of fold changes, if fold changes data is used as the metric (group 1 / group 2 or group 2 / group 1). This option only affects the fold changes retrieved from the fold changes calculation part of the app, from the currently running session. Fold changes data that is loaded from files is not affected. </p><p class="P8">Another option in the iGA settings subpanel is the choice of the direction of iGA. This affects whether the metrics are sorted in increasing or decreasing order. The increasing order is suited for p-values data, as the smallest p-values should be at the top of the list. The decreasing order is suited for fold changes data, as the highest fold changes should be at the top of the list.</p><p class="P7"> </p><p class="Standard_20__28_user_29_"><span class="T2">Server part</span></p><p class="P8">iGA [11] in the app is performed by the module named <span class="T3">running_iga_module</span>. As input, the module takes the <span class="T3">rtr</span> variable that contains test results from the app's current session. For running iGA, the module contains an object from <span class="T3">iga_calculator</span> class. The class contains the iGA function written by Francesco Del Carratore.</p><p class="Standard_20__28_user_29_"> </p><h2 class="Heading_20_2"><a id="a__Test_results"><span/></a><span class="T9">Test results</span></h2><p class="P8">All the test results pages in the app (except Volcano plot) have a dropdown menu at the top of the page which enables to select which test queue item's results are displayed. All the results pages contain data table(s) for numeric results and download buttons for the test statistics. If p-values are shown in the results table, the user has the choice to toggle whether to display the values that are corrected for multiple testing or to display the uncorrected values. Plots on the results pages depend on the type of experiment.</p><p class="Standard_20__28_user_29_"> </p><h3 class="Heading_20_3"><a id="a__t-test__F-test_and_Mann-Whitney_U_test"><span/></a>t-test, F-test and Mann-Whitney U test</h3><p class="Standard_20__28_user_29_"><span class="T2">User interface</span></p><p class="P8">The results page contains a data table with p-values for each variable. p-values smaller than 0.05 are highlighted in a different colour (green the results of the t-test and the Mann-Whitney U test, red in the results of the F-test). Clicking on the table opens a subpanel that shows full results for the variable that corresponds to the row that was clicked on. The subpanel also contains a box plot of the comparison of the values of the two test groups for the selected variable.</p><p class="P7"> </p><p class="Standard_20__28_user_29_"><span class="T2">Server part</span></p><p class="P8">The results pages of the t-test and the F-test are generated by two instances of the <span class="T3">two_groups_test_results_page</span> module. The module takes the <span class="T3">rtr</span> variable (that stores all test results) as one of its inputs. A second input to the module defines what type of test's results will be shown. The <span class="T3">two_groups_test_results_page</span> module calls two other modules <span class="T3">(two_groups_test_data_download</span> and <span class="T3">two_groups_test_data_one_variable_statistics_download</span>) that handle the downloading of test results.</p><p class="Standard_20__28_user_29_"> </p><h3 class="Heading_20_3"><a id="a__Shapiro-Wilk_test"><span/></a>Shapiro-Wilk test</h3><p class="Standard_20__28_user_29_"><span class="T2">User interface</span></p><p class="P8">On the page of viewing Shapiro-Wilk test results, p-values are shown in a table. The values are separate for the two test groups. By clicking on the p-values table, it is possible to view the Shapiro-Wilk statistic of each result element.</p><p class="P10"> </p><p class="Standard_20__28_user_29_"><span class="T2">Server part</span></p><p class="P8">Shapiro-Wilk test modules are displayed using the module called <span class="T3">viewing_shapiro_test_results_module</span>. This module is relatively similar with the <span class="T3">two_groups_test_results_page</span> module and uses the same two accessory modules to manage the downloading of results.</p><p class="Standard_20__28_user_29_"> </p><h3 class="Heading_20_3"><a id="a__Fold_changes"><span/></a>Fold changes</h3><p class="Standard_20__28_user_29_"><span class="T2">User interface</span></p><p class="P8">Fold change values of group averages are shown in a table. Under the table, there are settings buttons that enable to change the direction of fold changes calculation and the scale of fold changes. For data with paired samples, there is a heat map that displays the fold changes of individual sample pairs.</p><p class="P7"> </p><p class="Standard_20__28_user_29_"><span class="T2">Server part</span></p><p class="P8">The fold changes calculation results are displayed by the <span class="T3">two_groups_fold_changes_results_page</span> module, which takes the test results data as input and contains all the components of the fold changes results page.</p><p class="Standard_20__28_user_29_"> </p><h3 class="Heading_20_3"><a id="a__Rank_product_and_rank_sum"><span/></a>Rank product and rank sum</h3><p class="Standard_20__28_user_29_"><span class="T2">User interface</span></p><p class="P8">The results pages of rank product and rank sum contain a table with p-values (group 1 vs group 2, group 2 vs group 1 and the minimum p-value for each variable). The results page also contains plots of the estimated percentage of false predictions (PFP).</p><p class="P7"> </p><p class="Standard_20__28_user_29_"><span class="T2">Server part</span></p><p class="P8">The rank sum and rank product results are displayed using<span class="T3"> rank_product_module</span> (which contains a data table and ggplot2 [24] plots) and <span class="T3">results_pages_selectinput_module</span> (which contains the dropdown menu for result page selection).</p><p class="Standard_20__28_user_29_"> </p><h3 class="Heading_20_3"><a id="a__iGA"><span/></a>iGA</h3><p class="Standard_20__28_user_29_"><span class="T2">User interface</span></p><p class="P8">The iGA [11] and db-iGA results pages contain a summary table of iGA statistics. The pages have the option to see a table of variables and their associated metrics, sorted by the metric value, where the variables belonging to each annotation class are highlighted in green. This table can help to get an overview of how the annotation classes are distributed among variables. The page also contain a plot of iGA or db-iGA results for all variables and annotation classes in the data. The plot shows the annotation classes for each variable. Besides this, the plot shows which variables in each annotation class were selected by iGA.</p><p class="P7"> </p><p class="Standard_20__28_user_29_"><span class="T2">Server part</span></p><p class="P8">iGA results are displayed by <span class="T3">iga_results_module</span>, which contains the functions for displaying the data tables and the ggplot2 [24] figure.</p><p class="Standard_20__28_user_29_"> </p><p class="Standard_20__28_user_29_"><span class="T4"></span></p><p class="Standard_20__28_user_29_"> </p><h3 class="Heading_20_3"><a id="a__Volcano_plot"><span/></a>Volcano plot</h3><p class="Standard_20__28_user_29_"><span class="T2">User interface</span></p><p class="P8">The volcano plot requires p-values and fold changes as its input data. Clicking the "Get data for volcano plot" button in the app retrieves p-values and fold changes that have been calculated in the same session of the app. If there are multiple sets of fold changes or p-values available, the choice of which ones to use for the volcano plot can be made by using the dropdown menus on the volcano plot page. Clicking the "Generate volcano plot" button will make the app display the volcano plot. In the settings subpanel of volcano plot, the user can modify the p-value statistical significance threshold, the minimum log2 fold change that is considered as differential expression, the direction of fold change calculation and whether the p-values are adjusted for multiple testing. If the settings are changed, the plot is updated when the "Refresh the volcano plot" button is clicked.</p><p class="P8">In the volcano plot, variables can be selected with mouse and the names of the selected variables will be displayed in a text box underneath the plot. Besides this, the user has the option to choose specific variables to be marked with arrows on the plot.</p><p class="P7"> </p><p class="Standard_20__28_user_29_"><span class="T2">Server part</span></p><p class="P8">Volcano plot is generated in the <span class="T3">volcano_plot_module</span>. A set of functions in this module and in the <span class="T3">two_groups_test_results_tracker</span> class serve to find the p-values and fold changes from statistical tests with matching test parameters. The volcano plot is drawn using Plotly [31].</p><p class="Standard_20__28_user_29_"> </p><h2 class="Heading_20_2"><a id="a__Saving_and_loading_R_workspace"><span/></a><p class="Standard_20__28_user_29_"> </p><h3 class="Heading_20_3"><a id="a__Saving_and_loading_R_workspace"><span/></a>Saving and loading R workspace</h3><p class="Standard_20__28_user_29_"><span class="T2">User interface</span></p><p class="P8">R workspace contains the variables and data that are stored in the memory of R when running an R session. The app has the options for saving the workspace of the current session in .Rdata format and loading a previously saved session.</p><p class="P7"> </p><p class="Standard_20__28_user_29_"><span class="T2">Server part</span></p><p class="P8">The saving and loading of R workspace is done by the modules called <span class="T3">saving_r_workspace_module</span> and <span class="T3">loading_r_workspace_module</span>. These modules use the R commands <span class="T3">save.image</span> and <span class="T3">load</span> for saving and loading the workspace, respectively.</p><h2 class="Heading_20_2"><a id="a__Help"><span/></a><span class="T9">Help</span></h2><p class="Standard_20__28_user_29_"><span class="T2">User interface</span></p><p class="P8">The help part of the app contains descriptions of the statistical methods used in the app, explanatory texts about the user interface of the app and a short description of the origins and aims of the app. For the most part, these texts overlap with the contents of this report.</p><p class="P7"> </p><p class="Standard_20__28_user_29_"><span class="T2">Server part</span></p><p class="P8">The help and about pages use three small modules named <span class="T3">app_help_module</span>, <span class="T3">statistical_methods_help_module</span> and <span class="T3">about_the_app_module</span> that load the help and about texts from HTML files.</p><p class="Standard_20__28_user_29_"> </p><h1 class="Heading_20_1"><a id="a__Discussion_and_Conclusions"><span/></a>Discussion and Conclusions</h1><p class="P8"></p><p class="Standard_20__28_user_29_"><a id="_GoBack"/></p><p class="Standard_20__28_user_29_"> </p><p class="Standard_20__28_user_29_"> </p><p class="P11">1<a id="_ENREF_1"/>.<span> Silva, L.B., et al., <span class="T1">General guidelines for biomedical software development.</span> F1000Res, 2017. <span class="T6">6</span>: p. 273.</span></p><p class="P11"><a id="_ENREF_2"/>2.<span> Gehlenborg, N., et al., <span class="T1">Visualization of omics data for systems biology.</span> Nat Methods, 2010. <span class="T6">7</span>(3 Suppl): p. S56-68.</span></p><p class="P11"><a id="_ENREF_3"/>3.<span> Gentleman, R.C., et al., <span class="T1">Bioconductor: open software development for computational biology and bioinformatics.</span> Genome Biol, 2004. <span class="T6">5</span>(10): p. R80.</span></p><p class="P11"><a id="_ENREF_4"/>4.<span> Microsoft_Corporation, <span class="T1">Microsoft Excel</span>,  2016: Redmond, Washington, USA. office.microsoft.com/en-us/excel</span></p><p class="P11"><a id="_ENREF_5"/>5.<span> GraphPad_Software, <span class="T1">Graphpad Prism</span>,  2017: La Jolla California USA. http://www.graphpad.com/</span></p><p class="P11"><a id="_ENREF_6"/>6.<span> IBM, <span class="T1">SPSS</span>,  2017: Armonk, New York, United States. www.ibm.com/us-en/marketplace/spss-statistics</span></p><p class="P11"><a id="_ENREF_7"/>7.<span> Pavelin, K., et al., <span class="T1">Bioinformatics meets user-centred design: a perspective.</span> PLoS Comput Biol, 2012. <span class="T6">8</span>(7): p. e1002554.</span></p><p class="P11"><a id="_ENREF_8"/>8.<span> Prlic, A. and J.B. Procter, <span class="T1">Ten simple rules for the open development of scientific software.</span> PLoS Comput Biol, 2012. <span class="T6">8</span>(12): p. e1002802.</span></p><p class="P11"><a id="_ENREF_9"/>9.<span> R_Development_Core_Team, <span class="T1">R: A language and environment for statistical computing</span>  2008, Vienna, Austria: R Foundation for Statistical Computing.</span></p><p class="P11"><a id="_ENREF_10"/>10.<span> Jimenez, R.C., et al., <span class="T1">Four simple recommendations to encourage best practices in research software.</span> F1000Res, 2017. <span class="T6">6</span>.</span></p><p class="P11"><a id="_ENREF_11"/>11.<span> Breitling, R., A. Amtmann, and P. Herzyk, <span class="T1">Iterative Group Analysis (iGA): a simple tool to enhance sensitivity and facilitate interpretation of microarray experiments.</span> BMC Bioinformatics, 2004. <span class="T6">5</span>: p. 34.</span></p><p class="P11"><a id="_ENREF_12"/>12.<span> Carratore, F.D., et al., <span class="T1">RankProd: Rank Product method for identifying differentially expressed genes with application in meta-analysis. R  package version 3.2.0.</span>, 2016.</span></p><p class="P11"><a id="_ENREF_13"/>13.<span> Laing, E. and C.P. Smith, <span class="T1">RankProdIt: A web-interactive Rank Products analysis tool.</span> BMC Res Notes, 2010. <span class="T6">3</span>: p. 221.</span></p><p class="P11"><a id="_ENREF_14"/>14.<span> Chang W, C.J., Allaire JJ, Xie Y, McPherson J. <span class="T1">shiny: Web Application Framework for R</span>.  2016; Available from: https://CRAN.R-project.org/package=shiny</span></p><p class="P11"><a id="_ENREF_15"/>15.<span> Hastings, J., K. Haug, and C. Steinbeck, <span class="T1">Ten recommendations for software engineering in research.</span> Gigascience, 2014. <span class="T6">3</span>(1): p. 31.</span></p><p class="P11"><a id="_ENREF_16"/>16.<span> Xia, J., et al., <span class="T1">MetaboAnalyst: a web server for metabolomic data analysis and interpretation.</span> Nucleic Acids Res, 2009. <span class="T6">37</span>(Web Server issue): p. W652-60.</span></p><p class="P11"><a id="_ENREF_17"/>17.<span> RStudio, <span class="T1">RStudio</span>,  2017: Boston, MA, USA. http://www.rstudio.org/</span></p><p class="P11"><a id="_ENREF_18"/>18.<span> Eich, B., <span class="T1">JavaScript</span>,  2017, Netscape Communications Corporation, Mozilla Foundation, Ecma International: Mountain View, California, USA. https://developer.mozilla.org/en-US/docs/Web/JavaScript</span></p><p class="P11"><a id="_ENREF_19"/>19.<span> W3C_&amp;_WHATWG, <span class="T1">Hypertext Markup Language (HTML)</span>,  2014. https://www.w3.org/html/</span></p><p class="P11"><a id="_ENREF_20"/>20.<span> Beeley, C., <span class="T1">Web Application Development with R Using Shiny</span>  2016: Packt Publishing.</span></p><p class="P11"><a id="_ENREF_21"/>21.<span> Cheng, J., <span class="T1">Modularizing Shiny app code</span>,  2015, Shiny by RStudio<span class="T10">.</span> https://shiny.rstudio.com/articles/modules.html.</span></p><p class="P11"><a id="_ENREF_22"/>22.<span> Bolstad, B.M., <span class="T1">preprocessCore: A collection of pre-processing functions. R package version  1.38.1.</span>, 2017.</span></p><p class="P11"><a id="_ENREF_23"/>23.<span> Wilke, C.O., <span class="T1">cowplot: Streamlined Plot Theme and Plot Annotations for 'ggplot2'. R package version 0.8.0</span>, 2017.</span></p><p class="P11"><a id="_ENREF_24"/>24.<span> Wickham, H., <span class="T1">ggplot2: Elegant Graphics for Data Analysis</span>  2016, New York, USA: Springer-Verlag.</span></p><p class="P11"><a id="_ENREF_25"/>25.<span> Huber, W., et al., <span class="T1">Variance stabilization applied to microarray data calibration and to the quantification of differential expression.</span> Bioinformatics, 2002. <span class="T6">18 Suppl 1</span>: p. S96-104.</span></p><p class="P11"><a id="_ENREF_26"/>26.<span> Ren, K., <span class="T1">rlist: A Toolbox for Non-Tabular Data Manipulation. R package version 0.4.6.1</span>, 2016.</span></p><p class="P11"><a id="_ENREF_27"/>27.<span> Zhang, Z., <span class="T1">Reshaping and aggregating data: an introduction to reshape package.</span> Ann Transl Med, 2016. <span class="T6">4</span>(4): p. 78.</span></p><p class="P11"><a id="_ENREF_28"/>28.<span> Bailey, E., <span class="T1">shinyBS: Twitter Bootstrap Components for Shiny. R package version 0.61.</span> 2015.</span></p><p class="P11"><a id="_ENREF_29"/>29.<span> Tang, Y.H., M.; Li, W., <span class="T1">ggfortify: Unified Interface to Visualize Statistical Result of  Popular R Packages.</span> The R Journal, 2016.</span></p><p class="P11"><a id="_ENREF_30"/>30.<span> Jolliffe, I.T., <span class="T1">Principal Component Analysis</span>. 2 ed. Springer Series in Statistics  2002, New York,  USA: Springer-Verlag New York.</span></p><p class="P11"><a id="_ENREF_31"/>31.<span> Sievert, C., et al., <span class="T1">plotly: Create Interactive Web Graphics via 'plotly.js'. R package version 4.7.1.</span>, 2017.</span></p><p class="P11"><a id="_ENREF_32"/>32.<span> Maechler, M., <span class="T1">Rmpfr: R MPFR - Multiple Precision Floating-Point Reliable. R package version 0.6-1.</span>, 2016.</span></p><p class="P11"><a id="_ENREF_33"/>33.<span> Attali, D., <span class="T1">shinyjs: Easily Improve the User Experience of Your Shiny Apps in Seconds. R package version  0.9.1.</span>, 2017.</span></p><p class="P11"><a id="_ENREF_34"/>34.<span> Warnes, G.R., et al., <span class="T1">gplots: Various R Programming Tools for Plotting Data. R  package version 3.0.1.</span> 2016.</span></p><p class="P11"><a id="_ENREF_35"/>35.<span> Rocke, D., et al., <span class="T1">LMGene: LMGene Software for Data Transformation and</span></span></p><p class="P11"><span class="T1">Identification of Differentially Expressed Genes in Gene Expression Arrays. R package version 2.32.0. </span>, 2013.</p><p class="P11"><a id="_ENREF_36"/>36.<span> Wickham, H., et al., <span class="T1">dplyr: A Grammar of Data Manipulation. R  package version 0.7.2.</span>, 2017.</span></p><p class="P11"><a id="_ENREF_37"/>37.<span> Dowle, M. and A. Srinivasan, <span class="T1">data.table: Extension of `data.frame`. R package version 1.10.4.</span> 2017.</span></p><p class="P11"><a id="_ENREF_38"/>38.<span> Wickham, H., <span class="T1">The Split-Apply-Combine Strategy for Data Analysis.</span> Journal of Statistical Software, 2011. <span class="T6">0 (1), 1-29</span>.</span></p><p class="P11"><a id="_ENREF_39"/>39.<span> Xie, Y., <span class="T1">DT: A Wrapper of the JavaScript Library 'DataTables', R package version 0.2</span>, 2016.</span></p><p class="P11"><a id="_ENREF_40"/>40.<span> Gandy, D. <span class="T1">http://fontawesome.io/</span>.  2016; Available from: http://fontawesome.io/</span></p><p class="P11"><a id="_ENREF_41"/>41.<span> The_Inkscape_Team, <span class="T1">Inkscape 0.92</span>,  2017. inkscape.org</span></p><p class="P11"><a id="_ENREF_42"/>42.<span> van Rossum, G., <span class="T1">Python tutorial, Technical Report CS-R9526, Centrum voor Wiskunde en Informatica (CWI), Amsterdam.</span> 1995.</span></p><p class="P11"><a id="_ENREF_43"/>43.<span> Daffertshofer, A., et al., <span class="T1">PCA in studying coordination and variability: a tutorial.</span> Clin Biomech (Bristol, Avon), 2004. <span class="T6">19</span>(4): p. 415-28.</span></p><p class="P11"><a id="_ENREF_44"/>44.<span> Golub, G.H. and C.F. van Loan, <span class="T1">Matrix computations</span>  1990, Baltimore, USA: John Hopkins University Press.</span></p><p class="P11"><a id="_ENREF_45"/>45.<span> Joliffe, I.T. and B.J. Morgan, <span class="T1">Principal component analysis and exploratory factor analysis.</span> Stat Methods Med Res, 1992. <span class="T6">1</span>(1): p. 69-95.</span></p><p class="P11"><a id="_ENREF_46"/>46.<span> Sherlock, G., <span class="T1">Analysis of large-scale gene expression data.</span> Curr Opin Immunol, 2000. <span class="T6">12</span>(2): p. 201-5.</span></p><p class="P11"><a id="_ENREF_47"/>47.<span> Everitt, B., <span class="T1">Cluster analysis</span>  1974, the University of Michigan, USA: Heinemann Educational [for] the Social Science Research Council.</span></p><p class="P11"><a id="_ENREF_48"/>48.<span> Wu, J., <span class="T1">Advances in K-means Clustering</span>  2012: Springer-Verlag Berlin Heidelberg.</span></p><p class="P11"><a id="_ENREF_49"/>49.<span> Boddy, R. and G. Smith, <span class="T1">Statistical Methods in Practice: for Scientists and Technologists</span>  2009: Wiley. 246.</span></p><p class="P11"><a id="_ENREF_50"/>50.<span> Crawley, M.J., <span class="T1">The R Book</span>  2007: John Wiley &amp; Sons. 950.</span></p><p class="P11"><a id="_ENREF_51"/>51.<span> Sunitha, L., et al., <span class="T1">Automatic Outlier Identification in Data Mining Using IQR in Real-Time Data.</span> International Journal of Advanced Research in Computer and Communication Engineering, 2014. <span class="T6">3</span>(6).</span></p><p class="P11"><a id="_ENREF_52"/>52.<span> Albright, S.C., W. Winston, and C. Zappe, <span class="T1">Data Analysis and Decision Making</span>  2010: Cengage Learning. 1080.</span></p><p class="P11"><a id="_ENREF_53"/>53.<span> Hayes, A.F., <span class="T1">Introduction to Mediation, Moderation, and Conditional Process Analysis: A Regression-Based Approach</span>. Methodology in the Social Sciences Series  2013: Guilford Press. 507.</span></p><p class="P11"><a id="_ENREF_54"/>54.<span> Varmuza, K. and P. Filzmoser, <span class="T1">Introduction to Multivariate Statistical Analysis in Chemometrics</span>. illustrated ed  2016: CRC Press. 336.</span></p><p class="P11"><a id="_ENREF_55"/>55.<span> van den Berg, R.A., et al., <span class="T1">Centering, scaling, and transformations: improving the biological information content of metabolomics data.</span> BMC Genomics, 2006. <span class="T6">7</span>: p. 142.</span></p><p class="P11"><a id="_ENREF_56"/>56.<span> Ivosev, G., L. Burton, and R. Bonner, <span class="T1">Dimensionality reduction and visualization in principal component analysis.</span> Anal Chem, 2008. <span class="T6">80</span>(13): p. 4933-44.</span></p><p class="P11"><a id="_ENREF_57"/>57.<span> Bolstad, B.M., et al., <span class="T1">A comparison of normalization methods for high density oligonucleotide array data based on variance and bias.</span> Bioinformatics, 2003. <span class="T6">19</span>(2): p. 185-93.</span></p><p class="P11"><a id="_ENREF_58"/>58.<span> Do, J.H. and D.K. Choi, <span class="T1">Normalization of microarray data: single-labeled and dual-labeled arrays.</span> Mol Cells, 2006. <span class="T6">22</span>(3): p. 254-61.</span></p><p class="P11"><a id="_ENREF_59"/>59.<span> Hu, J. and X. He, <span class="T1">Enhanced quantile normalization of microarray data to reduce loss of information in gene expression profiles.</span> Biometrics, 2007. <span class="T6">63</span>(1): p. 50-9.</span></p><p class="P11"><a id="_ENREF_60"/>60.<span> McDonald, J.H., <span class="T1">Handbook of Biological Statistics</span>. 3rd ed. ed  2014, Baltimore, Maryland, USA: Sparky House Publishing.</span></p><p class="P11"><a id="_ENREF_61"/>61.<span> Motakis, E.S., et al., <span class="T1">Variance stabilization and normalization for one-color microarray data using a data-driven multiscale approach.</span> Bioinformatics, 2006. <span class="T6">22</span>(20): p. 2547-53.</span></p><p class="P11"><a id="_ENREF_62"/>62.<span> Parsons, H.M., et al., <span class="T1">Improved classification accuracy in 1- and 2-dimensional NMR metabolomics data using the variance stabilising generalised logarithm transformation.</span> BMC Bioinformatics, 2007. <span class="T6">8</span>: p. 234.</span></p><p class="P11"><a id="_ENREF_63"/>63.<span> Durbin, B.P., et al., <span class="T1">A variance-stabilizing transformation for gene-expression microarray data.</span> Bioinformatics, 2002. <span class="T6">18 Suppl 1</span>: p. S105-10.</span></p><p class="P11"><a id="_ENREF_64"/>64.<span> Christensen, R., <span class="T1">Analysis of Variance, Design, and Regression: Applied Statistical Methods</span>. Chapman &amp; Hall/CRC Texts in Statistical Science  1996: CRC Press. 608.</span></p><p class="P11"><a id="_ENREF_65"/>65.<span> Ghasemi, A. and S. Zahediasl, <span class="T1">Normality tests for statistical analysis: a guide for non-statisticians.</span> Int J Endocrinol Metab, 2012. <span class="T6">10</span>(2): p. 486-9.</span></p><p class="P11"><a id="_ENREF_66"/>66.<span> Royston, P., <span class="T1">An extension of Shapiro and Wilk's W test for normality to large samples.</span> Applied Statistics, 1982(31): p. 115–124.</span></p><p class="P11"><a id="_ENREF_67"/>67.<span> Royston, P., <span class="T1">Approximating the Shapiro-Wilk W-test for non-normality.</span> Statistics and Computing, 1992(2): p. 117-119.</span></p><p class="P11"><a id="_ENREF_68"/>68.<span> Razali, N.M. and Y.B. Wah, <span class="T1">Power Comparisons of Shapiro-Wilk, Kolmogorov-Smirnov, Lilliefors and Anderson-Darling Tests.</span> Journal of Statistical Modeling and Analytics, 2011. <span class="T6">2</span>(1): p. 21-33.</span></p><p class="P11"><a id="_ENREF_69"/>69.<span> Macfie, B.P. and P.M. Nufrio, <span class="T1">Applied Statistics for Public Policy</span>  2006: M.E. Sharpe.</span></p><p class="P11"><a id="_ENREF_70"/>70.<span> Alfassi, Z.B., Z. Boger, and Y. Ronen, <span class="T1">Statistical Treatment of Analytical Data</span>  2005: Blackwell Science.</span></p><p class="P11"><a id="_ENREF_71"/>71.<span> LeBlanc, D.C., <span class="T1">Statistics: Concepts and Applications for Science</span>  2004: Jones and Bartlett.</span></p><p class="P11"><a id="_ENREF_72"/>72.<span> Wissing, D.R. and D. Timm, <span class="T1">Statistics for the nonstatistician: Part I.</span> South Med J, 2012. <span class="T6">105</span>(3): p. 126-30.</span></p><p class="P11"><a id="_ENREF_73"/>73.<span> du Prel, J.B., et al., <span class="T1">Choosing statistical tests: part 12 of a series on evaluation of scientific publications.</span> Dtsch Arztebl Int, 2010. <span class="T6">107</span>(19): p. 343-8.</span></p><p class="P11"><a id="_ENREF_74"/>74.<span> Motulsky, H., <span class="T1">Intuitive Biostatistics 1st Edition</span>  1995, New York, USA: Oxford University Press.</span></p><p class="P11"><a id="_ENREF_75"/>75.<span> Blann, A.D. and B.R. Nation, <span class="T1">Good analytical practice: statistics and handling data in biomedical science. A primer and directions for authors. Part 1: Introduction. Data within and between one or two sets of individuals.</span> Br J Biomed Sci, 2008. <span class="T6">65</span>(4): p. 209-17.</span></p><p class="P11"><a id="_ENREF_76"/>76.<span> Chap, T.L., <span class="T1">Introductory Biostatistics</span>  2003, Hoboken, New Jersey: John Wiley &amp; Sons, Inc.</span></p><p class="P11"><a id="_ENREF_77"/>77.<span> Jan, S.L. and G. Shieh, <span class="T1">Optimal sample sizes for Welch's test under various allocation and cost considerations.</span> Behav Res Methods, 2011. <span class="T6">43</span>(4): p. 1014-22.</span></p><p class="P11"><a id="_ENREF_78"/>78.<span> Declare, M.L., D., Leys, C., <span class="T1">Why Psychologists Should by Default Use Welch’s t-test Instead of Student’s t-test.</span> International Review of Social Psychology, 2017. <span class="T6">30(1)</span>: p. 92–101.</span></p><p class="P11"><a id="_ENREF_79"/>79.<span> Sinha, P.P., <span class="T1">Bioinformatics with R Cookbook</span>  2014: Packt Publishing.</span></p><p class="P11"><a id="_ENREF_80"/>80.<span> Gohlmann, H. and W. Talloen, <span class="T1">Gene Expression Studies Using Affymetrix Microarrays</span>  2009: CRC Press.</span></p><p class="P11"><a id="_ENREF_81"/>81.<span> Breitling, R., et al., <span class="T1">Rank products: a simple, yet powerful, new method to detect differentially regulated genes in replicated microarray experiments.</span> FEBS Lett, 2004. <span class="T6">573</span>(1-3): p. 83-92.</span></p><p class="P11"><a id="_ENREF_82"/>82.<span> Heskes, T., R. Eisinga, and R. Breitling, <span class="T1">A fast algorithm for determining bounds and accurate approximate p-values of the rank product statistic for replicate experiments.</span> BMC Bioinformatics, 2014. <span class="T6">15</span>: p. 367.</span></p><p class="P11"><a id="_ENREF_83"/>83.<span> Eisinga, R., R. Breitling, and T. Heskes, <span class="T1">The exact probability distribution of the rank product statistics for replicated experiments.</span> FEBS Lett, 2013. <span class="T6">587</span>(6): p. 677-82.</span></p><p class="P11"><a id="_ENREF_84"/>84.<span> Breitling, R. and P. Herzyk, <span class="T1">Rank-based methods as a non-parametric alternative of the T-statistic for the analysis of biological microarray data.</span> J Bioinform Comput Biol, 2005. <span class="T6">3</span>(5): p. 1171-89.</span></p><p class="P11"><a id="_ENREF_85"/>85.<span> Wiederhold, E., et al., <span class="T1">The yeast vacuolar membrane proteome.</span> Mol Cell Proteomics, 2009. <span class="T6">8</span>(2): p. 380-92.</span></p><p class="P11"><a id="_ENREF_86"/>86.<span> Krawetz, S., <span class="T1">Bioinformatics for Systems Biology</span>  2008: Humana Press.</span></p><p class="P11"><a id="_ENREF_87"/>87.<span> Kelly, S. <span class="T1">Plot.ly Volcano Plot Example</span>.  2016; Available from: https://stevekm.github.io/2016/09/24/Plot.ly-Volcano-Plot.html</span></p><p class="P11"><a id="_ENREF_88"/>88.<span> Hamadeh, H.K. and C.A. Afshari, <span class="T1">Toxicogenomics: Principles and Applications</span>  2004: Wiley.</span></p><p class="P11"><a id="_ENREF_89"/>89.<span> Lang, T.A. and M. Secic, <span class="T1">How to Report Statistics in Medicine: Annotated Guidelines for Authors, Editors, and Reviewers</span>  2006: American College of Physicians.</span></p><p class="P11"><a id="_ENREF_90"/>90.<span> Harris, M.A., et al., <span class="T1">The Gene Ontology (GO) database and informatics resource.</span> Nucleic Acids Res, 2004. <span class="T6">32</span>(Database issue): p. D258-61.</span></p><p class="P11"><a id="_ENREF_91"/>91.<span> Haslwanter, T., <span class="T1">An Introduction to Statistics with Python: With Applications in the Life Sciences</span>  2016: Springer International Publishing.</span></p><p class="P11"><a id="_ENREF_92"/>92.<span> Django_core_team, <span class="T1">Django: A Web framework for the Python programming language</span>,  2011, Django Software Foundation: Lawrence, Kansas, USA. http://www.djangoproject.com </span></p><p class="P11"><a id="_ENREF_93"/>93.<span> Brattli, D., <span class="T1">Python Reactive Programming</span>. Kindle Edition ed  2017: Packt Publishing. 416.</span></p><p class="P11"><a id="_ENREF_94"/>94.<span> Saternos, C., <span class="T1">Client-Server Web Apps with JavaScript and Java: Rich, Scalable, and RESTful</span>  2014: O'Reilly Media.</span></p><p class="P11"><a id="_ENREF_95"/>95.<span> University_of_Ljubljana, <span class="T1">Orange</span>,  2017. http://orange.biolab.si</span></p><p class="P11"><a id="_ENREF_96"/>96.<span> KNIME.com, <span class="T1">KNIME - The Open Analytics Platform</span>,  2017. http://www.knime.org</span></p><p class="P11"><a id="_ENREF_97"/>97.<span> van der Maaten, L. and G. Hinton, <span class="T1">Visualizing Data using t-SNE.</span> Journal of Machine Learning Research, 2008(9): p. 2579–2605.</span></p><p class="Standard_20__28_user_29_"> </p><p class="Standard_20__28_user_29_"> </p>
